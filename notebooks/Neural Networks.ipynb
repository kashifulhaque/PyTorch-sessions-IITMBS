{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline and dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an image transform pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = v2.Compose([\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Lambda(lambda x: torch.flatten(x)) # Can also use x.view(-1) tp flatten the tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = \"train_data\",\n",
    "                            train = True,\n",
    "                            download = True,\n",
    "                            transform = image_transform)\n",
    "\n",
    "\n",
    "test_data = datasets.MNIST(root = \"test_data\",\n",
    "                           train = False,\n",
    "                           download = True,\n",
    "                           transform = image_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dataloader to create batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=1000,  shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([1000, 784])\n",
      "Labels batch shape: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (feed_forward_network): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feed_forward_network = nn.Sequential(\n",
    "            nn.Linear(784, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        y_hat = self.feed_forward_network(X)\n",
    "        return y_hat\n",
    "\n",
    "model = NeuralNetwork().to(device= device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (feed_forward_network): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: feed_forward_network.0.weight | Size: torch.Size([100, 784]) | Values : tensor([[ 0.0330,  0.0290, -0.0036,  ..., -0.0196, -0.0143, -0.0068],\n",
      "        [-0.0155,  0.0064, -0.0143,  ...,  0.0185,  0.0013,  0.0106]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: feed_forward_network.0.bias | Size: torch.Size([100]) | Values : tensor([0.0204, 0.0289], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: feed_forward_network.2.weight | Size: torch.Size([100, 100]) | Values : tensor([[ 0.0940, -0.0844, -0.0394,  0.0300,  0.0991, -0.0177, -0.0741, -0.0685,\n",
      "         -0.0540, -0.0691, -0.0669, -0.0996, -0.0411, -0.0494,  0.0208,  0.0152,\n",
      "         -0.0460,  0.0720, -0.0130, -0.0175,  0.0106,  0.0565,  0.0146, -0.0995,\n",
      "         -0.0403,  0.0935, -0.0318,  0.0200, -0.0517, -0.0603, -0.0842, -0.0525,\n",
      "          0.0793, -0.0834, -0.0506,  0.0299, -0.0662, -0.0265, -0.0273, -0.0483,\n",
      "          0.0224,  0.0932,  0.0816, -0.0425, -0.0669,  0.0964,  0.0013,  0.0094,\n",
      "          0.0699,  0.0400,  0.0300,  0.0392,  0.0327,  0.0410, -0.0183, -0.0479,\n",
      "         -0.0167,  0.0952, -0.0109,  0.0007, -0.0181, -0.0513, -0.0135,  0.0370,\n",
      "         -0.0381, -0.0852,  0.0333, -0.0129, -0.0630,  0.0062,  0.0963, -0.0452,\n",
      "         -0.0350, -0.0284,  0.0757, -0.0517, -0.0299,  0.0266,  0.0260,  0.0414,\n",
      "          0.0031, -0.0170, -0.0401,  0.0475, -0.0422,  0.0273,  0.0715,  0.0377,\n",
      "          0.0178,  0.0377, -0.0623, -0.0862,  0.0394, -0.0502,  0.0768,  0.0871,\n",
      "         -0.0697,  0.0760,  0.0213, -0.0155],\n",
      "        [-0.0810, -0.0567, -0.0803,  0.0486, -0.0473, -0.0138, -0.0952, -0.0851,\n",
      "         -0.0913, -0.0742,  0.0204,  0.0508, -0.0216, -0.0435, -0.0915,  0.0939,\n",
      "         -0.0146,  0.0995,  0.0644, -0.0381,  0.0159,  0.0006,  0.0500,  0.0924,\n",
      "         -0.0430, -0.0009,  0.0105,  0.0764,  0.0436,  0.0168,  0.0877,  0.0851,\n",
      "          0.0502, -0.0118, -0.0678,  0.0806,  0.0163,  0.0096, -0.0975,  0.0555,\n",
      "         -0.0513,  0.0238,  0.0613, -0.0480,  0.0332, -0.0605,  0.0855,  0.0929,\n",
      "          0.0140,  0.0547,  0.0592, -0.0627, -0.0190, -0.0905, -0.0302,  0.0128,\n",
      "          0.0049,  0.0321,  0.0947,  0.0319,  0.0895, -0.0985, -0.0100,  0.0765,\n",
      "          0.0432,  0.0823, -0.0690, -0.0188, -0.0239,  0.0155, -0.0143, -0.0746,\n",
      "         -0.0958,  0.0634, -0.0551, -0.0842,  0.0941,  0.0352,  0.0683, -0.0060,\n",
      "          0.0302, -0.0377,  0.0525,  0.0706,  0.0760,  0.0881,  0.0125,  0.0992,\n",
      "          0.0024,  0.0776,  0.0756,  0.0493, -0.0307,  0.0373, -0.0133, -0.0619,\n",
      "         -0.0227,  0.0334,  0.0846, -0.0564]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: feed_forward_network.2.bias | Size: torch.Size([100]) | Values : tensor([ 0.0442, -0.0748], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: feed_forward_network.4.weight | Size: torch.Size([10, 100]) | Values : tensor([[ 0.0663,  0.0393, -0.0292, -0.0734, -0.0005,  0.0925,  0.0163,  0.0447,\n",
      "          0.0010,  0.0258,  0.0565,  0.0266,  0.0211,  0.0148,  0.0708, -0.0518,\n",
      "          0.0446, -0.0445, -0.0528, -0.0552,  0.0656,  0.0078,  0.0923, -0.0488,\n",
      "          0.0684, -0.0706, -0.0328, -0.0435,  0.0827,  0.0986, -0.0807,  0.0368,\n",
      "          0.0898,  0.0186, -0.0594, -0.0707, -0.0825,  0.0172, -0.0243, -0.0223,\n",
      "         -0.0038,  0.0458,  0.0708, -0.0362,  0.0329,  0.0325,  0.0044, -0.0475,\n",
      "         -0.0745, -0.0330, -0.0344, -0.0533, -0.0806,  0.0888, -0.0278,  0.0181,\n",
      "          0.0097, -0.0986, -0.0624,  0.0279, -0.0916,  0.0620, -0.0145,  0.0999,\n",
      "         -0.0187,  0.0914,  0.0521,  0.0792, -0.0155,  0.0554, -0.0799, -0.0470,\n",
      "          0.0835,  0.0947, -0.0039,  0.0584, -0.0465, -0.0075,  0.0799, -0.0052,\n",
      "          0.0075, -0.0215, -0.0443,  0.0952,  0.0424,  0.0665, -0.0455, -0.0168,\n",
      "         -0.0720,  0.0112,  0.0645,  0.0208,  0.0831,  0.0651,  0.0423, -0.0334,\n",
      "         -0.0011,  0.0762, -0.0163, -0.0348],\n",
      "        [-0.0735, -0.0552, -0.0040, -0.0283, -0.0324, -0.0797, -0.0135, -0.0799,\n",
      "          0.0141,  0.0662, -0.0669,  0.0855,  0.0931, -0.0234, -0.0379,  0.0190,\n",
      "         -0.0906, -0.0002,  0.0620, -0.0649, -0.0346, -0.0397, -0.0196,  0.0597,\n",
      "          0.0578,  0.0733, -0.0243, -0.0332, -0.0485,  0.0284,  0.0353,  0.0965,\n",
      "          0.0204,  0.0327,  0.0474,  0.0739,  0.0110,  0.0863,  0.0213,  0.0575,\n",
      "          0.0307,  0.0395,  0.0226,  0.0845, -0.0600,  0.0410,  0.0789,  0.0264,\n",
      "         -0.0387, -0.0037,  0.0965,  0.0119,  0.0068, -0.0601,  0.0146, -0.0588,\n",
      "          0.0885, -0.0251, -0.0114,  0.0001, -0.0315, -0.0120, -0.0908,  0.0988,\n",
      "         -0.0425,  0.0418, -0.0311,  0.0872, -0.0419,  0.0610,  0.0519,  0.0679,\n",
      "         -0.0041,  0.0481, -0.0533, -0.0927,  0.0397,  0.0763,  0.0261, -0.0352,\n",
      "         -0.0075,  0.0910, -0.0897, -0.0254, -0.0115,  0.0072, -0.0915,  0.0583,\n",
      "          0.0448, -0.0117,  0.0170,  0.0563,  0.0703, -0.0835,  0.0239, -0.0086,\n",
      "          0.0140,  0.0103, -0.0449, -0.0075]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: feed_forward_network.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0227, -0.0772], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X------------------Epoch : 1/100------------------X\n",
      "Training Avg loss: 2.298409\n",
      "Testing Avg loss: 2.268742 \n",
      "Accuracy: 25.3% \n",
      "\n",
      "X------------------Epoch : 2/100------------------X\n",
      "Training Avg loss: 2.254421\n",
      "Testing Avg loss: 2.235836 \n",
      "Accuracy: 37.2% \n",
      "\n",
      "X------------------Epoch : 3/100------------------X\n",
      "Training Avg loss: 2.224627\n",
      "Testing Avg loss: 2.206972 \n",
      "Accuracy: 43.5% \n",
      "\n",
      "X------------------Epoch : 4/100------------------X\n",
      "Training Avg loss: 2.196158\n",
      "Testing Avg loss: 2.177841 \n",
      "Accuracy: 48.0% \n",
      "\n",
      "X------------------Epoch : 5/100------------------X\n",
      "Training Avg loss: 2.166557\n",
      "Testing Avg loss: 2.146546 \n",
      "Accuracy: 52.2% \n",
      "\n",
      "X------------------Epoch : 6/100------------------X\n",
      "Training Avg loss: 2.134478\n",
      "Testing Avg loss: 2.113151 \n",
      "Accuracy: 56.5% \n",
      "\n",
      "X------------------Epoch : 7/100------------------X\n",
      "Training Avg loss: 2.099983\n",
      "Testing Avg loss: 2.077372 \n",
      "Accuracy: 56.6% \n",
      "\n",
      "X------------------Epoch : 8/100------------------X\n",
      "Training Avg loss: 2.062876\n",
      "Testing Avg loss: 2.039095 \n",
      "Accuracy: 59.0% \n",
      "\n",
      "X------------------Epoch : 9/100------------------X\n",
      "Training Avg loss: 2.023127\n",
      "Testing Avg loss: 1.997735 \n",
      "Accuracy: 61.1% \n",
      "\n",
      "X------------------Epoch : 10/100------------------X\n",
      "Training Avg loss: 1.980586\n",
      "Testing Avg loss: 1.953363 \n",
      "Accuracy: 62.8% \n",
      "\n",
      "X------------------Epoch : 11/100------------------X\n",
      "Training Avg loss: 1.934809\n",
      "Testing Avg loss: 1.905881 \n",
      "Accuracy: 63.8% \n",
      "\n",
      "X------------------Epoch : 12/100------------------X\n",
      "Training Avg loss: 1.885871\n",
      "Testing Avg loss: 1.855418 \n",
      "Accuracy: 66.2% \n",
      "\n",
      "X------------------Epoch : 13/100------------------X\n",
      "Training Avg loss: 1.834142\n",
      "Testing Avg loss: 1.802271 \n",
      "Accuracy: 67.5% \n",
      "\n",
      "X------------------Epoch : 14/100------------------X\n",
      "Training Avg loss: 1.780259\n",
      "Testing Avg loss: 1.747415 \n",
      "Accuracy: 69.5% \n",
      "\n",
      "X------------------Epoch : 15/100------------------X\n",
      "Training Avg loss: 1.724922\n",
      "Testing Avg loss: 1.691426 \n",
      "Accuracy: 71.9% \n",
      "\n",
      "X------------------Epoch : 16/100------------------X\n",
      "Training Avg loss: 1.668530\n",
      "Testing Avg loss: 1.635004 \n",
      "Accuracy: 73.4% \n",
      "\n",
      "X------------------Epoch : 17/100------------------X\n",
      "Training Avg loss: 1.611766\n",
      "Testing Avg loss: 1.578321 \n",
      "Accuracy: 75.0% \n",
      "\n",
      "X------------------Epoch : 18/100------------------X\n",
      "Training Avg loss: 1.555052\n",
      "Testing Avg loss: 1.521996 \n",
      "Accuracy: 76.5% \n",
      "\n",
      "X------------------Epoch : 19/100------------------X\n",
      "Training Avg loss: 1.498846\n",
      "Testing Avg loss: 1.466354 \n",
      "Accuracy: 77.3% \n",
      "\n",
      "X------------------Epoch : 20/100------------------X\n",
      "Training Avg loss: 1.443557\n",
      "Testing Avg loss: 1.411801 \n",
      "Accuracy: 78.9% \n",
      "\n",
      "X------------------Epoch : 21/100------------------X\n",
      "Training Avg loss: 1.389591\n",
      "Testing Avg loss: 1.358381 \n",
      "Accuracy: 80.3% \n",
      "\n",
      "X------------------Epoch : 22/100------------------X\n",
      "Training Avg loss: 1.336734\n",
      "Testing Avg loss: 1.306560 \n",
      "Accuracy: 80.9% \n",
      "\n",
      "X------------------Epoch : 23/100------------------X\n",
      "Training Avg loss: 1.285530\n",
      "Testing Avg loss: 1.256366 \n",
      "Accuracy: 81.9% \n",
      "\n",
      "X------------------Epoch : 24/100------------------X\n",
      "Training Avg loss: 1.236477\n",
      "Testing Avg loss: 1.208425 \n",
      "Accuracy: 83.0% \n",
      "\n",
      "X------------------Epoch : 25/100------------------X\n",
      "Training Avg loss: 1.189324\n",
      "Testing Avg loss: 1.162338 \n",
      "Accuracy: 83.7% \n",
      "\n",
      "X------------------Epoch : 26/100------------------X\n",
      "Training Avg loss: 1.144247\n",
      "Testing Avg loss: 1.118927 \n",
      "Accuracy: 84.1% \n",
      "\n",
      "X------------------Epoch : 27/100------------------X\n",
      "Training Avg loss: 1.101393\n",
      "Testing Avg loss: 1.077133 \n",
      "Accuracy: 84.5% \n",
      "\n",
      "X------------------Epoch : 28/100------------------X\n",
      "Training Avg loss: 1.060644\n",
      "Testing Avg loss: 1.037845 \n",
      "Accuracy: 84.9% \n",
      "\n",
      "X------------------Epoch : 29/100------------------X\n",
      "Training Avg loss: 1.021897\n",
      "Testing Avg loss: 1.000501 \n",
      "Accuracy: 85.3% \n",
      "\n",
      "X------------------Epoch : 30/100------------------X\n",
      "Training Avg loss: 0.985157\n",
      "Testing Avg loss: 0.964688 \n",
      "Accuracy: 85.7% \n",
      "\n",
      "X------------------Epoch : 31/100------------------X\n",
      "Training Avg loss: 0.950376\n",
      "Testing Avg loss: 0.931445 \n",
      "Accuracy: 86.1% \n",
      "\n",
      "X------------------Epoch : 32/100------------------X\n",
      "Training Avg loss: 0.917516\n",
      "Testing Avg loss: 0.900034 \n",
      "Accuracy: 86.4% \n",
      "\n",
      "X------------------Epoch : 33/100------------------X\n",
      "Training Avg loss: 0.886489\n",
      "Testing Avg loss: 0.870332 \n",
      "Accuracy: 86.5% \n",
      "\n",
      "X------------------Epoch : 34/100------------------X\n",
      "Training Avg loss: 0.857040\n",
      "Testing Avg loss: 0.842152 \n",
      "Accuracy: 86.7% \n",
      "\n",
      "X------------------Epoch : 35/100------------------X\n",
      "Training Avg loss: 0.829233\n",
      "Testing Avg loss: 0.815514 \n",
      "Accuracy: 86.9% \n",
      "\n",
      "X------------------Epoch : 36/100------------------X\n",
      "Training Avg loss: 0.802909\n",
      "Testing Avg loss: 0.789680 \n",
      "Accuracy: 87.1% \n",
      "\n",
      "X------------------Epoch : 37/100------------------X\n",
      "Training Avg loss: 0.777950\n",
      "Testing Avg loss: 0.766011 \n",
      "Accuracy: 87.4% \n",
      "\n",
      "X------------------Epoch : 38/100------------------X\n",
      "Training Avg loss: 0.754396\n",
      "Testing Avg loss: 0.743286 \n",
      "Accuracy: 87.6% \n",
      "\n",
      "X------------------Epoch : 39/100------------------X\n",
      "Training Avg loss: 0.732006\n",
      "Testing Avg loss: 0.722587 \n",
      "Accuracy: 87.8% \n",
      "\n",
      "X------------------Epoch : 40/100------------------X\n",
      "Training Avg loss: 0.710943\n",
      "Testing Avg loss: 0.702381 \n",
      "Accuracy: 88.0% \n",
      "\n",
      "X------------------Epoch : 41/100------------------X\n",
      "Training Avg loss: 0.691071\n",
      "Testing Avg loss: 0.683963 \n",
      "Accuracy: 88.0% \n",
      "\n",
      "X------------------Epoch : 42/100------------------X\n",
      "Training Avg loss: 0.672213\n",
      "Testing Avg loss: 0.665854 \n",
      "Accuracy: 88.2% \n",
      "\n",
      "X------------------Epoch : 43/100------------------X\n",
      "Training Avg loss: 0.654232\n",
      "Testing Avg loss: 0.649149 \n",
      "Accuracy: 88.2% \n",
      "\n",
      "X------------------Epoch : 44/100------------------X\n",
      "Training Avg loss: 0.637354\n",
      "Testing Avg loss: 0.633185 \n",
      "Accuracy: 88.4% \n",
      "\n",
      "X------------------Epoch : 45/100------------------X\n",
      "Training Avg loss: 0.621226\n",
      "Testing Avg loss: 0.618362 \n",
      "Accuracy: 88.6% \n",
      "\n",
      "X------------------Epoch : 46/100------------------X\n",
      "Training Avg loss: 0.606170\n",
      "Testing Avg loss: 0.604285 \n",
      "Accuracy: 88.6% \n",
      "\n",
      "X------------------Epoch : 47/100------------------X\n",
      "Training Avg loss: 0.591687\n",
      "Testing Avg loss: 0.590679 \n",
      "Accuracy: 88.8% \n",
      "\n",
      "X------------------Epoch : 48/100------------------X\n",
      "Training Avg loss: 0.578027\n",
      "Testing Avg loss: 0.578075 \n",
      "Accuracy: 89.0% \n",
      "\n",
      "X------------------Epoch : 49/100------------------X\n",
      "Training Avg loss: 0.565095\n",
      "Testing Avg loss: 0.565943 \n",
      "Accuracy: 89.0% \n",
      "\n",
      "X------------------Epoch : 50/100------------------X\n",
      "Training Avg loss: 0.552872\n",
      "Testing Avg loss: 0.554600 \n",
      "Accuracy: 89.1% \n",
      "\n",
      "X------------------Epoch : 51/100------------------X\n",
      "Training Avg loss: 0.541216\n",
      "Testing Avg loss: 0.543603 \n",
      "Accuracy: 89.0% \n",
      "\n",
      "X------------------Epoch : 52/100------------------X\n",
      "Training Avg loss: 0.530148\n",
      "Testing Avg loss: 0.533396 \n",
      "Accuracy: 89.3% \n",
      "\n",
      "X------------------Epoch : 53/100------------------X\n",
      "Training Avg loss: 0.519655\n",
      "Testing Avg loss: 0.523421 \n",
      "Accuracy: 89.5% \n",
      "\n",
      "X------------------Epoch : 54/100------------------X\n",
      "Training Avg loss: 0.509487\n",
      "Testing Avg loss: 0.514291 \n",
      "Accuracy: 89.6% \n",
      "\n",
      "X------------------Epoch : 55/100------------------X\n",
      "Training Avg loss: 0.499885\n",
      "Testing Avg loss: 0.505070 \n",
      "Accuracy: 89.6% \n",
      "\n",
      "X------------------Epoch : 56/100------------------X\n",
      "Training Avg loss: 0.490624\n",
      "Testing Avg loss: 0.496576 \n",
      "Accuracy: 89.6% \n",
      "\n",
      "X------------------Epoch : 57/100------------------X\n",
      "Training Avg loss: 0.481616\n",
      "Testing Avg loss: 0.488878 \n",
      "Accuracy: 89.6% \n",
      "\n",
      "X------------------Epoch : 58/100------------------X\n",
      "Training Avg loss: 0.473176\n",
      "Testing Avg loss: 0.480911 \n",
      "Accuracy: 89.7% \n",
      "\n",
      "X------------------Epoch : 59/100------------------X\n",
      "Training Avg loss: 0.465172\n",
      "Testing Avg loss: 0.473628 \n",
      "Accuracy: 89.9% \n",
      "\n",
      "X------------------Epoch : 60/100------------------X\n",
      "Training Avg loss: 0.457330\n",
      "Testing Avg loss: 0.467120 \n",
      "Accuracy: 89.9% \n",
      "\n",
      "X------------------Epoch : 61/100------------------X\n",
      "Training Avg loss: 0.449790\n",
      "Testing Avg loss: 0.460198 \n",
      "Accuracy: 90.0% \n",
      "\n",
      "X------------------Epoch : 62/100------------------X\n",
      "Training Avg loss: 0.442652\n",
      "Testing Avg loss: 0.453785 \n",
      "Accuracy: 90.0% \n",
      "\n",
      "X------------------Epoch : 63/100------------------X\n",
      "Training Avg loss: 0.435507\n",
      "Testing Avg loss: 0.447466 \n",
      "Accuracy: 90.1% \n",
      "\n",
      "X------------------Epoch : 64/100------------------X\n",
      "Training Avg loss: 0.428909\n",
      "Testing Avg loss: 0.441665 \n",
      "Accuracy: 90.1% \n",
      "\n",
      "X------------------Epoch : 65/100------------------X\n",
      "Training Avg loss: 0.422615\n",
      "Testing Avg loss: 0.435938 \n",
      "Accuracy: 90.2% \n",
      "\n",
      "X------------------Epoch : 66/100------------------X\n",
      "Training Avg loss: 0.416466\n",
      "Testing Avg loss: 0.430170 \n",
      "Accuracy: 90.3% \n",
      "\n",
      "X------------------Epoch : 67/100------------------X\n",
      "Training Avg loss: 0.410464\n",
      "Testing Avg loss: 0.425808 \n",
      "Accuracy: 90.3% \n",
      "\n",
      "X------------------Epoch : 68/100------------------X\n",
      "Training Avg loss: 0.404685\n",
      "Testing Avg loss: 0.420320 \n",
      "Accuracy: 90.2% \n",
      "\n",
      "X------------------Epoch : 69/100------------------X\n",
      "Training Avg loss: 0.399185\n",
      "Testing Avg loss: 0.415667 \n",
      "Accuracy: 90.4% \n",
      "\n",
      "X------------------Epoch : 70/100------------------X\n",
      "Training Avg loss: 0.393855\n",
      "Testing Avg loss: 0.411168 \n",
      "Accuracy: 90.4% \n",
      "\n",
      "X------------------Epoch : 71/100------------------X\n",
      "Training Avg loss: 0.388632\n",
      "Testing Avg loss: 0.406935 \n",
      "Accuracy: 90.5% \n",
      "\n",
      "X------------------Epoch : 72/100------------------X\n",
      "Training Avg loss: 0.383678\n",
      "Testing Avg loss: 0.402365 \n",
      "Accuracy: 90.6% \n",
      "\n",
      "X------------------Epoch : 73/100------------------X\n",
      "Training Avg loss: 0.378660\n",
      "Testing Avg loss: 0.397544 \n",
      "Accuracy: 90.7% \n",
      "\n",
      "X------------------Epoch : 74/100------------------X\n",
      "Training Avg loss: 0.374051\n",
      "Testing Avg loss: 0.393823 \n",
      "Accuracy: 90.8% \n",
      "\n",
      "X------------------Epoch : 75/100------------------X\n",
      "Training Avg loss: 0.369374\n",
      "Testing Avg loss: 0.390049 \n",
      "Accuracy: 90.8% \n",
      "\n",
      "X------------------Epoch : 76/100------------------X\n",
      "Training Avg loss: 0.364951\n",
      "Testing Avg loss: 0.385803 \n",
      "Accuracy: 90.9% \n",
      "\n",
      "X------------------Epoch : 77/100------------------X\n",
      "Training Avg loss: 0.360619\n",
      "Testing Avg loss: 0.383075 \n",
      "Accuracy: 90.8% \n",
      "\n",
      "X------------------Epoch : 78/100------------------X\n",
      "Training Avg loss: 0.356530\n",
      "Testing Avg loss: 0.378973 \n",
      "Accuracy: 90.9% \n",
      "\n",
      "X------------------Epoch : 79/100------------------X\n",
      "Training Avg loss: 0.352605\n",
      "Testing Avg loss: 0.375601 \n",
      "Accuracy: 91.0% \n",
      "\n",
      "X------------------Epoch : 80/100------------------X\n",
      "Training Avg loss: 0.348697\n",
      "Testing Avg loss: 0.372703 \n",
      "Accuracy: 91.0% \n",
      "\n",
      "X------------------Epoch : 81/100------------------X\n",
      "Training Avg loss: 0.344894\n",
      "Testing Avg loss: 0.369563 \n",
      "Accuracy: 90.9% \n",
      "\n",
      "X------------------Epoch : 82/100------------------X\n",
      "Training Avg loss: 0.341281\n",
      "Testing Avg loss: 0.367243 \n",
      "Accuracy: 90.9% \n",
      "\n",
      "X------------------Epoch : 83/100------------------X\n",
      "Training Avg loss: 0.337614\n",
      "Testing Avg loss: 0.364032 \n",
      "Accuracy: 91.0% \n",
      "\n",
      "X------------------Epoch : 84/100------------------X\n",
      "Training Avg loss: 0.334153\n",
      "Testing Avg loss: 0.361372 \n",
      "Accuracy: 91.1% \n",
      "\n",
      "X------------------Epoch : 85/100------------------X\n",
      "Training Avg loss: 0.330889\n",
      "Testing Avg loss: 0.358127 \n",
      "Accuracy: 91.2% \n",
      "\n",
      "X------------------Epoch : 86/100------------------X\n",
      "Training Avg loss: 0.327651\n",
      "Testing Avg loss: 0.355483 \n",
      "Accuracy: 91.1% \n",
      "\n",
      "X------------------Epoch : 87/100------------------X\n",
      "Training Avg loss: 0.324365\n",
      "Testing Avg loss: 0.352284 \n",
      "Accuracy: 91.3% \n",
      "\n",
      "X------------------Epoch : 88/100------------------X\n",
      "Training Avg loss: 0.321287\n",
      "Testing Avg loss: 0.350169 \n",
      "Accuracy: 91.3% \n",
      "\n",
      "X------------------Epoch : 89/100------------------X\n",
      "Training Avg loss: 0.318128\n",
      "Testing Avg loss: 0.347642 \n",
      "Accuracy: 91.3% \n",
      "\n",
      "X------------------Epoch : 90/100------------------X\n",
      "Training Avg loss: 0.315236\n",
      "Testing Avg loss: 0.345478 \n",
      "Accuracy: 91.4% \n",
      "\n",
      "X------------------Epoch : 91/100------------------X\n",
      "Training Avg loss: 0.312507\n",
      "Testing Avg loss: 0.342666 \n",
      "Accuracy: 91.4% \n",
      "\n",
      "X------------------Epoch : 92/100------------------X\n",
      "Training Avg loss: 0.309754\n",
      "Testing Avg loss: 0.340617 \n",
      "Accuracy: 91.4% \n",
      "\n",
      "X------------------Epoch : 93/100------------------X\n",
      "Training Avg loss: 0.307081\n",
      "Testing Avg loss: 0.338955 \n",
      "Accuracy: 91.5% \n",
      "\n",
      "X------------------Epoch : 94/100------------------X\n",
      "Training Avg loss: 0.304247\n",
      "Testing Avg loss: 0.336730 \n",
      "Accuracy: 91.4% \n",
      "\n",
      "X------------------Epoch : 95/100------------------X\n",
      "Training Avg loss: 0.301994\n",
      "Testing Avg loss: 0.334152 \n",
      "Accuracy: 91.5% \n",
      "\n",
      "X------------------Epoch : 96/100------------------X\n",
      "Training Avg loss: 0.299259\n",
      "Testing Avg loss: 0.332964 \n",
      "Accuracy: 91.6% \n",
      "\n",
      "X------------------Epoch : 97/100------------------X\n",
      "Training Avg loss: 0.297009\n",
      "Testing Avg loss: 0.330846 \n",
      "Accuracy: 91.6% \n",
      "\n",
      "X------------------Epoch : 98/100------------------X\n",
      "Training Avg loss: 0.294569\n",
      "Testing Avg loss: 0.328809 \n",
      "Accuracy: 91.5% \n",
      "\n",
      "X------------------Epoch : 99/100------------------X\n",
      "Training Avg loss: 0.292149\n",
      "Testing Avg loss: 0.327044 \n",
      "Accuracy: 91.6% \n",
      "\n",
      "X------------------Epoch : 100/100------------------X\n",
      "Training Avg loss: 0.290025\n",
      "Testing Avg loss: 0.325659 \n",
      "Accuracy: 91.5% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Training Mode\n",
    "    datasize = len(train_dataloader.dataset)\n",
    "    train_loss, num_batches = 0, len(train_dataloader)\n",
    "    print(f'X------------------Epoch : {epoch+1}/{epochs}------------------X')\n",
    "    # Training Loop for each batch\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Clearing the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Feedforward\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= num_batches\n",
    "    print(f'Training Avg loss: {train_loss:>8f}')\n",
    "    \n",
    "    # Running inference for each epoch\n",
    "    model.eval() # Evaluation mode\n",
    "    datasize = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # Ensures no gradients are computed during evaluation mode.\n",
    "        for X, y in test_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(X)\n",
    "            test_loss += loss_fn(y_hat, y).item()\n",
    "            correct += (y_hat.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= datasize\n",
    "    print(f\"Testing Avg loss: {test_loss:>8f} \\nAccuracy: {(100*correct):>0.1f}% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (feed_forward_network): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model, \"torch_model.pt\")\n",
    "\n",
    "# Load\n",
    "new_model = torch.load(\"torch_model.pt\")\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the parameters\n",
    "\n",
    "The above code saves the entire model\n",
    "\n",
    "Instead we can save the model parameters and the optimizer parameters seperately in the form of dictionary using `state_dict()`. This allows us to view and update them easily\n",
    "\n",
    "Source: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "feed_forward_network.0.weight \t torch.Size([100, 784])\n",
      "feed_forward_network.0.bias \t torch.Size([100])\n",
      "feed_forward_network.2.weight \t torch.Size([100, 100])\n",
      "feed_forward_network.2.bias \t torch.Size([100])\n",
      "feed_forward_network.4.weight \t torch.Size([10, 100])\n",
      "feed_forward_network.4.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}, 2: {'momentum_buffer': None}, 3: {'momentum_buffer': None}, 4: {'momentum_buffer': None}, 5: {'momentum_buffer': None}}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading model paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (feed_forward_network): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"torch_model_params.pt\")\n",
    "torch.save(optimizer.state_dict(), \"optimizer_params.pt\")\n",
    "\n",
    "new_model = NeuralNetwork()\n",
    "new_model.load_state_dict(torch.load(\"torch_model_params.pt\"))\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can save model and optimizer parameters, we can use them as checkpoints during the training loop like adding this piece of code.\n",
    "\n",
    "```python\n",
    "if (epoch % 5 == 0):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "    }, \"checkpoint.pt.tar\") # Tar extension allows us to save multiple components in a single file.\n",
    "```\n",
    "This code allows us to save the parameters every 5 epochs. However do keep in mind that using same filename everytime we save the checkpoint overrides the previous checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Onnx to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input\n",
    "dummy_input = torch.randn(100, 28*28, dtype=torch.float32).to(device)\n",
    "onnx_program = torch.onnx.dynamo_export(model, dummy_input)\n",
    "onnx_program.save(\"torch_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
